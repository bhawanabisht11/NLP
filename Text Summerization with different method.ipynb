{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f42b0bd-93ca-426f-8b67-5ba7006f4356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\", quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d8db867-68cf-43fb-b320-6e604aee46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "OpenAI released a new model for language tasks, improving accuracy and efficiency across benchmarks.\n",
    "Analysts say the upgrade could reduce inference costs for enterprises and unlock new applications in education, healthcare, and customer support.\n",
    "However, some researchers warn about risks around misinformation and bias, urging better evaluations and safety tooling.\n",
    "Investors reacted positively, with several AI stocks gaining during afternoon trading.\n",
    "The company also announced partnerships with universities to study the model's impact on learning outcomes.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb634bd-a4f7-49b0-ba4d-7e6e2c71d21b",
   "metadata": {},
   "source": [
    "# Quick Baseline: LEAD-k (news-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12ac7d0-8b17-4f64-8a17-fde3334d141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Bhanu\n",
      "[nltk_data]     Bisht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Bhanu\n",
      "[nltk_data]     Bisht\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edabc40c-1a61-42b7-a6c2-3fff830d6d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OpenAI released a new model for language tasks, improving accuracy and efficiency across benchmarks. Analysts say the upgrade could reduce inference costs for enterprises and unlock new applications in education, healthcare, and customer support.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def lead_k_summary(text, k=3):\n",
    "    sents = sent_tokenize(text)\n",
    "    return \" \".join(sents[:k])\n",
    "\n",
    "print(lead_k_summary(text, k=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40247b77-ad2b-4302-b302-419546494edd",
   "metadata": {},
   "source": [
    "# 3) Extractive: TextRank (TF-IDF + PageRank)\n",
    "\n",
    "Idea:\n",
    "\n",
    "Split into sentences.\n",
    "\n",
    "Vectorize sentences with TF-IDF.\n",
    "\n",
    "Build a similarity graph (cosine similarity).\n",
    "\n",
    "Run PageRank to score sentences.\n",
    "\n",
    "Select top-N in original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a6f6c2-933d-42c9-af79-ced3183e8fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI released a new model for language tasks, improving accuracy and efficiency across benchmarks. Analysts say the upgrade could reduce inference costs for enterprises and unlock new applications in education, healthcare, and customer support. The company also announced partnerships with universities to study the model's impact on learning outcomes.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "def textrank_summarize(text, num_sentences=3, min_sent_len=20):\n",
    "    sents = [s.strip() for s in sent_tokenize(text)]\n",
    "    if len(sents) <= num_sentences:\n",
    "        return text.strip()\n",
    "    \n",
    "    # Filter very short sentences (often noisy)\n",
    "    kept = [s for s in sents if len(s) >= min_sent_len]\n",
    "    if len(kept) < num_sentences:\n",
    "        kept = sents  # fall back to all\n",
    "    \n",
    "    # TF-IDF sentence vectors\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform(kept)\n",
    "    \n",
    "    # Similarity graph\n",
    "    sim = cosine_similarity(X)\n",
    "    # Remove self-similarity\n",
    "    for i in range(sim.shape[0]):\n",
    "        sim[i, i] = 0.0\n",
    "    \n",
    "    # PageRank\n",
    "    graph = nx.from_numpy_array(sim)\n",
    "    scores = nx.pagerank(graph)\n",
    "    \n",
    "    # Rank sentences by score\n",
    "    ranked = sorted(((scores[i], i, sent) for i, sent in enumerate(kept)), reverse=True)\n",
    "    top = sorted(ranked[:num_sentences], key=lambda x: x[1])  # keep original order\n",
    "    \n",
    "    return \" \".join(s for _, _, s in top)\n",
    "\n",
    "print(textrank_summarize(text, num_sentences=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbaf8ae-6748-4e3b-b400-3fbc3140c272",
   "metadata": {},
   "source": [
    "## Make TextRank a function you can reuse on any text list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17eb370a-7a99-4955-ab75-8a53e4368fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_batch_textrank(docs, n=3):\n",
    "    return [textrank_summarize(d, num_sentences=n) for d in docs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d871e335-f353-4a6c-8c62-b7b193afa561",
   "metadata": {},
   "source": [
    "# 4) Abstractive: Transformer (DistilBART)\n",
    "\n",
    "This needs transformers + a backend (torch is common). DistilBART is lightweight and good quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7891d25c-8cbe-4c5a-8ef5-0a1dd9ac97f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Analysts say the upgrade could reduce inference costs for enterprises and unlock new applications in education, healthcare, and customer support . However, some researchers warn about risks around misinformation and bias .\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Small, fast summarizer model\n",
    "#MODEL = \"sshleifer/distilbart-cnn-12-6\"\n",
    "summarizer = pipeline(\"summarization\",  model=\"sshleifer/distilbart-cnn-12-6\", framework=\"pt\")  # uses PyTorch by default\n",
    "\n",
    "def bart_summarize(text, max_words=120, min_words=40):\n",
    "    # DistilBART is trained with tokens—roughly map words→tokens ~1:1 for short texts\n",
    "    # Tune these two until you like the output length.\n",
    "    result = summarizer(\n",
    "        text,\n",
    "        max_length=max_words,   # upper bound (tokens)\n",
    "        min_length=min_words,   # lower bound (tokens)\n",
    "        do_sample=False,        # deterministic (greedy/beam)\n",
    "        truncation=True\n",
    "    )\n",
    "    return result[0][\"summary_text\"]\n",
    "\n",
    "print(bart_summarize(text, max_words=80, min_words=30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75f62edc-6bab-4388-8647-97f87cad3594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f0700bf8cc41a2bb7fdd9b3c5da2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:  77%|#######7  | 944M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5115708a1e90433d8e00d5c55224d241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679851c38e314d8a8eb16f0ca53c4253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70c3fe61d68431ca09e6aaa36658aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d61bc2f60eb4fdca298b6cd471e47b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", framework=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1f0cc-487e-49aa-af03-f8fc7a43367c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6140486b-c772-4153-95e4-a28b493ed207",
   "metadata": {},
   "source": [
    "# 5) Long documents: chunking + 2-pass summarization\n",
    "\n",
    "Most encoder-decoder models accept ~1024 tokens. For longer inputs:\n",
    "\n",
    "Split into sentence-based chunks by token budget.\n",
    "\n",
    "Summarize each chunk.\n",
    "\n",
    "Summarize the concatenation of chunk summaries (“summary of summaries”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c004bdfa-ea38-468e-877b-8c9c4c6b550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "def chunk_by_tokens(text, max_tokens=900):\n",
    "    sents = sent_tokenize(text)\n",
    "    chunk, count = [], 0\n",
    "    for s in sents:\n",
    "        t = tokenizer.encode(s, add_special_tokens=False)\n",
    "        if count + len(t) <= max_tokens:\n",
    "            chunk.append(s)\n",
    "            count += len(t)\n",
    "        else:\n",
    "            yield \" \".join(chunk)\n",
    "            chunk, count = [s], len(t)\n",
    "    if chunk:\n",
    "        yield \" \".join(chunk)\n",
    "\n",
    "def summarize_long_text(text, per_chunk_max=140, per_chunk_min=60, final_max=120, final_min=40):\n",
    "    chunks = list(chunk_by_tokens(text, max_tokens=900))\n",
    "    if len(chunks) == 1:\n",
    "        return bart_summarize(chunks[0], max_words=final_max, min_words=final_min)\n",
    "    partials = [bart_summarize(c, max_words=per_chunk_max, min_words=per_chunk_min) for c in chunks]\n",
    "    mega = \" \".join(partials)\n",
    "    return bart_summarize(mega, max_words=final_max, min_words=final_min)\n",
    "\n",
    "# Example:\n",
    "# long_summary = summarize_long_text(very_long_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350c928-5eed-434e-982a-080017406b8a",
   "metadata": {},
   "source": [
    "# 6) Compare methods quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b87366f6-b9d5-4c73-9ee1-db85c4eabf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEAD-2:\n",
      " \n",
      "OpenAI released a new model for language tasks, improving accuracy and efficiency across benchmarks. Analysts say the upgrade could reduce inference costs for enterprises and unlock new applications in education, healthcare, and customer support. \n",
      "\n",
      "TextRank (3):\n",
      " OpenAI released a new model for language tasks, improving accuracy and efficiency across benchmarks. Analysts say the upgrade could reduce inference costs for enterprises and unlock new applications in education, healthcare, and customer support. The company also announced partnerships with universities to study the model's impact on learning outcomes. \n",
      "\n",
      "Abstractive (BART):\n",
      "  Analysts say the upgrade could reduce inference costs for enterprises and unlock new applications in education, healthcare, and customer support . However, some researchers warn about risks around misinformation and bias .\n"
     ]
    }
   ],
   "source": [
    "print(\"LEAD-2:\\n\", lead_k_summary(text, 2), \"\\n\")\n",
    "print(\"TextRank (3):\\n\", textrank_summarize(text, 3), \"\\n\")\n",
    "print(\"Abstractive (BART):\\n\", bart_summarize(text, 80, 30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab843b2-b74e-4b27-a038-462cca754f76",
   "metadata": {},
   "source": [
    "# 7) Evaluate with ROUGE (when you have reference summaries)\n",
    "\n",
    "If you have gold/reference summaries, compute ROUGE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd41787d-37e6-45c2-933a-2c9809927605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE: {'rouge1': 0.3529, 'rouge2': 0.1224, 'rougeL': 0.3529}\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def rouge_all(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    # return F1s for readability\n",
    "    return {k: round(v.fmeasure, 4) for k, v in scores.items()}\n",
    "\n",
    "# Example:\n",
    "ref = \"OpenAI released a model; analysts expect lower costs and new applications. Some warn about risks; stocks rose; universities to study impact.\"\n",
    "cand = bart_summarize(text, 80, 30)\n",
    "print(\"ROUGE:\", rouge_all(ref, cand))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e230337-3cea-4cb2-a8b1-ccdde55825a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b13544-47e6-45bb-9be5-1332edc21158",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TF 3.10)",
   "language": "python",
   "name": "tf310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
